{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import  Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.feature import NGram, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line(row):\n",
    "    if row is None:\n",
    "        return None\n",
    "    row = row.split('\\t')\n",
    "    rurl = row[0]\n",
    "    activity = row[1]\n",
    "    label = row[2]\n",
    "    activity = re.sub('[^A-Za-z_]', '', activity)\n",
    "    activity = re.sub(' +', ' ', activity.replace('_', ' ').strip())\n",
    "    return rurl, activity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(sc, data_path):\n",
    "    if sc is None or data_path is None:\n",
    "        return None\n",
    "    documents = sc.textFile(data_path).map(lambda line : read_line(line))\n",
    "    schema = StructType([StructField(\"rurl\", StringType(), True),\n",
    "                         StructField(\"activity\", StringType(), True),\n",
    "                         StructField(\"label\", StringType(), True)])\n",
    "    documents = SQLContext(sc).createDataFrame(documents, schema)  \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(documents):\n",
    "    if documents is None:\n",
    "        return None\n",
    "    tokenizer = Tokenizer(inputCol=\"activity\", outputCol=\"tokens\")\n",
    "    documents = tokenizer.transform(documents)\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "    documents = remover.transform(documents)\n",
    "    ngram = NGram(n=2, inputCol=\"filtered_tokens\", outputCol=\"bi-grams\")\n",
    "    documents = ngram.transform(documents)\n",
    "    cv = CountVectorizer(inputCol=\"bi-grams\", outputCol=\"features\")\n",
    "    cvModel = cv.fit(documents)\n",
    "    documents = cvModel.transform(documents)\n",
    "    documents = documents.withColumn(\"label\", documents[\"label\"].cast(\"double\"))\n",
    "    return documents, cvModel.vocabulary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(documents):\n",
    "    if documents is None:\n",
    "        return None\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=200)\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .build()\n",
    "    cross_val = CrossValidator(estimator=lr, estimatorParamMaps = paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds= 5)\n",
    "    cvModel = cross_val.fit(documents)\n",
    "    return cvModel.bestModel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metric(model, documents, output_path):\n",
    "    if model is None or documents is None or output_path is None:\n",
    "        return None\n",
    "    metricdf = pd.DataFrame({'auc': []})\n",
    "    model_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    predictions = model.transform(documents)\n",
    "    metric = model_eval.evaluate(predictions)\n",
    "    metricdf = metricdf.append({'auc': metric}, ignore_index=True)\n",
    "    metricdf = SQLContext(sc).createDataFrame(metricdf)\n",
    "    metricdf.coalesce(1).write.format('csv').option('delimiter', '\\t').option('header', 'true').save(output_path + '/' + 'metric')\n",
    "    return metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(sc, model, output_path):\n",
    "    if model is None or sc is None or output_path is None:\n",
    "        return None\n",
    "    # Save and load model\n",
    "    # model.save(sc, model_path)\n",
    "    # same_model = MatrixFactorizationModel.load(sc, model_path)\n",
    "    output_path = output_path + '/' + 'model'\n",
    "    model.save(output_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_weigths(model, output_path):\n",
    "    if model is None or output_path is None:\n",
    "        return None\n",
    "    weights = model.coefficients\n",
    "    modelweightsdf = SQLContext(sc).createDataFrame(weights.toArray().tolist(), \"float\")\n",
    "    modelweightsdf = modelweightsdf.withColumnRenamed(\"value\", \"model_weights\")\n",
    "    modelweightsdf = modelweightsdf.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id()))-1)\n",
    "    modelvocabdf = SQLContext(sc).createDataFrame(vocab, \"string\")\n",
    "    modelvocabdf = modelvocabdf.withColumnRenamed(\"value\", \"model_vocab\")\n",
    "    modelvocabdf = modelvocabdf.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id()))-1)\n",
    "    modeldf = modelvocabdf.join(modelweightsdf, \"index\", \"outer\").drop(\"index\")\n",
    "    modeldf.coalesce(1).write.format('csv').option('delimiter', '\\t').option('header', 'true').save(output_path + '/' + 'model_weights')\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=SparkConf().setAppName(\"category_classifier_on_spark\"))\n",
    "documents = read_data(sc, 'test')\n",
    "documents, vocab = transform_data(documents)\n",
    "trainDF, testDF = documents.randomSplit([0.8,0.2])\n",
    "model = run_model(trainDF)\n",
    "metric = extract_metric(model, testDF, 'output')\n",
    "write_model(sc, model, 'output')\n",
    "write_model_weigths(model, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_logistic_regression(sc, input_path, output_path):\n",
    "#     if sc is None or input_path is None or output_path is None:\n",
    "#         return None\n",
    "#     documents = read_data(sc, input_path)\n",
    "#     documents, vocab = transform_data(documents)\n",
    "#     trainDF, testDF = documents.randomSplit([0.8,0.2])\n",
    "#     model = run_model(trainDF)\n",
    "#     metric = extract_metric(model, testDF, output_path)\n",
    "#     write_model(sc, model, output_path)\n",
    "#     write_model_weigths(model, output_path)\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     if args is None:\n",
    "#         return None\n",
    "#     sc = SparkContext(conf = SparkConf().setAppName('LR_on_spark').set('spark.driver.maxResultSize', '4g'))\n",
    "#     run_logistic_regression(sc, args.input_path, args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--input_path', help='input_path', default='input_path')\n",
    "#     parser.add_argument('--output_path', help='output_path', default='output_path')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
